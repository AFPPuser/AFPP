
# ----------------
# Chip parameters
# ----------------
init_chip.n_cells_per_wl = 9000


# ----------------
# Agent parameters
# ---------------
AFPPAgent.n_pulses = 12
AFPPAgent.targets = [0.275 , 0.370875, 0.466625]  # normalized
AFPPAgent.model_cls = @AFPPRNN
AFPPAgent.env_handler = @CellsAboveTargetState  #@AboveTargetLearningReadPoint
AFPPAgent.delta_verify = 0.01

AFPPRNN.pulse_layers_size = [64, 64, 64]
AFPPRNN.verify_layers_size = [64, 64, 64, 64]
AFPPRNN.critic_layer_size = [64, 64, 64]
AFPPRNN.non_linearity_module = @ReLU
AFPPRNN.hidden_size = 8
AFPPRNN.num_layers = 2
AFPPRNN.rnn_type = @RNN  # @LSTM


# ----------------
# TrainManager parameters
# ----------------
#TrainManager.debug_mode = True

PPOTrainManager.wl_idx = 0.27  # normalized
PPOTrainManager.num_episodes = 1000000
PPOTrainManager.debug_mode = False
PPOTrainManager.batch_size = 6
PPOTrainManager.max_episodes = 20
PPOTrainManager.n_update_epochs = 1
PPOTrainManager.lr = 5e-4
PPOTrainManager.ckpt_freq = 100
PPOTrainManager.init_agent_from_ckpt = None
PPOTrainManager.reward_symmetry_factor = 0.1
PPOTrainManager.entropy_alpha = 0.1
PPOTrainManager.policy_clip = 0.3
PPOTrainManager.n_batches_per_epoch = 6
PPOTrainManager.decay_lr = 5000
PPOTrainManager.decay_asymmetry = 5000
PPOTrainManager.decay_entropy = 5000


PPOTrainManager.min_l_2 = 0.013  # Normalized
